{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be extracting and pre-processing all the data that we will need to train our neural network.\n",
    "We have the following data sets:\n",
    "- Trump tweets before announcing he was running  (http://trumptwitterarchive.com)\n",
    "    - 25967 tweets\n",
    "- A collection of tweets made by regular users in 2010 (https://archive.org/details/twitter_cikm_2010)\n",
    "    - 25967 tweets\n",
    "- Barack Obama's tweet story until March 2017 (https://community.periscopedata.com/t/x1fy7p/barack-obamas-tweet-history)\n",
    "     - 6735 tweets\n",
    "- Tweets made by Democrat politicians as of May 2018(https://www.kaggle.com/kapastor/democratvsrepublicantweets)\n",
    "     - 42068 tweets\n",
    " \n",
    "The original dataset used didn't contain any politicians besides Trump, which ended up causing a lot of false positives when mentioning any political issues, or America. That's why Obama and Democrat tweets were added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data from Trump tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_data=[]\n",
    "json_data = json.load(open('data/condensed_2009.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2010.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2011.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2012.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2013.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2014.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])\n",
    "json_data = json.load(open('data/condensed_2015.json'))\n",
    "for tweet in json_data:\n",
    "    trump_data.append(tweet[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we clean up the data. We remove links and create a \"LINK\" token. We remove Twitter mentions and create a \"MENT\" token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,tweet in enumerate(trump_data):\n",
    "    trump_data[index] = re.sub(r'https?:\\/\\/.*[\\r\\n]*', 'LINK', tweet, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,tweet in enumerate(trump_data):\n",
    "    trump_data[index] = re.sub(r'@[^\\s]+', 'MENT', tweet, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Trump data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract and process all the negative data points (non-Trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Democrat Tweets\n",
    "#Due to the large size of this file,  you will need to download from the previous Kaggle link and it to the /data subdirectory\n",
    "i=0\n",
    "non_trump_data = []\n",
    "with open('data/ExtractedTweets.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        if i == 0:\n",
    "            i+=1\n",
    "        else:\n",
    "            if row[0] == \"Democrat\":\n",
    "                non_trump_data.append(row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obama Tweets\n",
    "i=0\n",
    "with open('data/obama_tweets.csv') as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    for row in readCSV:\n",
    "        if i == 0:\n",
    "            i+=1\n",
    "        else:\n",
    "            non_trump_data.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regular User tweets\n",
    "i = 0\n",
    "user_dict = {}\n",
    "with open(\"data/training_set_tweets.txt\") as f:\n",
    "    while i<len(trump_data):\n",
    "        line = next(f)\n",
    "        split_line = line.split('\\t')\n",
    "        if split_line[0] not in user_dict and len(split_line) >= 3:\n",
    "            user_dict[split_line[0]] = 1\n",
    "            non_trump_data.append(split_line[2])\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process them the same way we did with Trump tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,tweet in enumerate(non_trump_data):\n",
    "    non_trump_data[index] = re.sub(r'https?:\\/\\/.*[\\r\\n]*', 'LINK', tweet, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,tweet in enumerate(non_trump_data):\n",
    "    non_trump_data[index] = re.sub(r'@[^\\s]+', 'MENT', tweet, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today, Senate Dems vote to #SaveTheInternet. Proud to support similar #NetNeutrality legislation here in the Houseâ€¦ LINK'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "non_trump_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add them together to create our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = trump_data + non_trump_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that we have a total of ~100k tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100736"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data to word count vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We willtokenize a our data set and build a vocabulary of known words using CountVectorizer. We will also save and use this vectorizer to apply it to the inputs on our model when we want to classify a new input.\n",
    "We set a max number of features (words) of 10k. We are not interested in words that appear rarely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "vectorizer.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the vocabulary we have extracted. Every word will have a unique identifier. We can see for example that trump has been assigned the number \"9126\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Large output. Uncomment if you want to take a look\n",
    "#print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now encode our training data using this vocabulary. This is a one hot encoding. We will take care of this later, as we prefer label based encoding as an input for our neural network (less inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_data = vectorizer.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index =dict([(value, key) for (key, value) in vectorizer.vocabulary_.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create the labels for our data set. 1 for a Trump tweet. 0 for not Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = [1 for _ in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trump_data), len(training_data)):\n",
    "    data_labels[i] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of one particular tweet, and how the encoded version looks (it's a 10k long array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"MENT MENT Thanks Donald. Now run for president! Fulfill your purpose! \"To much is given, much is required\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[25966]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_data[25966].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how it looks after we convert it to label based encoding. Every number corresponds to one word in the vocabulary we've seen before. We can see the number \"2818\", which corresponds to \"donald\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2818, 3669, 3781, 3915, 4834, 5674, 5924, 6174, 6925, 7152, 7502,\n",
       "        7744, 8914, 9035, 9973], dtype=int32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [sparse_row.indices for sparse_row in vector_data[25966]]\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's convert our one hot encoding to label based encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = [None for _ in range(len(training_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,one_hot in enumerate(vector_data):\n",
    "    label_data[index] = [sparse_row.indices for sparse_row in vector_data[index]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we'll pickle the data set, labels, and vectorizer so we can easily import it in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tweet_data'\n",
    "outfile = open(filename,'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(label_data,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data_labels'\n",
    "outfile = open(filename,'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data_labels,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"vectorizer\"\n",
    "outfile = open(filename, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectorizer,outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
